{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012352,
     "end_time": "2021-02-19T02:57:45.814025",
     "exception": false,
     "start_time": "2021-02-19T02:57:45.801673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extreme Fine Tuning of LGBM using Incremental training\n",
    "\n",
    "The purpose of this notebook is to implement the advanced LightGBM model training with a certain incremental approach. The trick is executed in following steps:\n",
    "\n",
    "* Find the best parameters for your LGBM, either manually or using optimization methods of your choice.\n",
    "* Train the model to the best RMSE you can get in one training round using high early stopping.\n",
    "* Train the model for 1 or 2 rounds with reduced learning rate.\n",
    "* Once the first few rounds are over, start reducing regularization params by a factor at each incremental training iteration, you will start observing improvements in 5th decimal place... which is enough to get 5th decimal improvement on your models leaderboard score.\n",
    "\n",
    "The inspiration for such an approach has been obtained from https://www.kaggle.com/awwalmalhi/extreme-fine-tuning-lgbm-using-7-step-training\n",
    "\n",
    "The initial settings for a manually tuned LightGBM model have been inherited from my ensembling kernel per https://www.kaggle.com/gvyshnya/ensemble-lgb-xgb-catboost-optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:45.841283Z",
     "iopub.status.busy": "2021-02-19T02:57:45.840312Z",
     "iopub.status.idle": "2021-02-19T02:57:47.963517Z",
     "shell.execute_reply": "2021-02-19T02:57:47.964045Z"
    },
    "papermill": {
     "duration": 2.138608,
     "end_time": "2021-02-19T02:57:47.964373",
     "exception": false,
     "start_time": "2021-02-19T02:57:45.825765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2021-02-21 11:55:39.405680\n"
     ]
    }
   ],
   "source": [
    "# main flow\n",
    "start_time = dt.datetime.now()\n",
    "print(\"Started at \", start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "in_kaggle = False\n",
    "\n",
    "\n",
    "def get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n",
    "    train_path = ''\n",
    "    test_path = ''\n",
    "    sample_submission_path = ''\n",
    "\n",
    "    if is_in_kaggle:\n",
    "        # running in Kaggle, inside the competition\n",
    "        train_path = '../input/tabular-playground-series-feb-2021/train.csv'\n",
    "        test_path = '../input/tabular-playground-series-feb-2021/test.csv'\n",
    "        sample_submission_path = '../input/tabular-playground-series-feb-2021/sample_submission.csv'\n",
    "    else:\n",
    "        # running locally\n",
    "        train_path = 'data/train.csv'\n",
    "        test_path = 'data/test.csv'\n",
    "        sample_submission_path = 'data/sample_submission.csv'\n",
    "\n",
    "    return train_path, test_path, sample_submission_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:47.992994Z",
     "iopub.status.busy": "2021-02-19T02:57:47.992059Z",
     "iopub.status.idle": "2021-02-19T02:57:52.182932Z",
     "shell.execute_reply": "2021-02-19T02:57:52.182337Z"
    },
    "papermill": {
     "duration": 4.206009,
     "end_time": "2021-02-19T02:57:52.183073",
     "exception": false,
     "start_time": "2021-02-19T02:57:47.977064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the training set and labels\n",
    "train_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n",
    "\n",
    "train = pd.read_csv(train_set_path)\n",
    "test = pd.read_csv(test_set_path)\n",
    "target = train.target\n",
    "\n",
    "subm = pd.read_csv(sample_subm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:52.219527Z",
     "iopub.status.busy": "2021-02-19T02:57:52.218831Z",
     "iopub.status.idle": "2021-02-19T02:57:52.291011Z",
     "shell.execute_reply": "2021-02-19T02:57:52.291579Z"
    },
    "papermill": {
     "duration": 0.095898,
     "end_time": "2021-02-19T02:57:52.291780",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.195882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['id', 'target'], axis=1)\n",
    "y_train = train.target\n",
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:52.319803Z",
     "iopub.status.busy": "2021-02-19T02:57:52.319147Z",
     "iopub.status.idle": "2021-02-19T02:57:52.324742Z",
     "shell.execute_reply": "2021-02-19T02:57:52.324193Z"
    },
    "papermill": {
     "duration": 0.020649,
     "end_time": "2021-02-19T02:57:52.324883",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.304234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [feature for feature in train.columns if 'cat' in feature]\n",
    "\n",
    "def label_encoder(df):\n",
    "    for feature in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:52.449340Z",
     "iopub.status.busy": "2021-02-19T02:57:52.375160Z",
     "iopub.status.idle": "2021-02-19T02:57:54.155477Z",
     "shell.execute_reply": "2021-02-19T02:57:54.154847Z"
    },
    "papermill": {
     "duration": 1.818185,
     "end_time": "2021-02-19T02:57:54.155645",
     "exception": false,
     "start_time": "2021-02-19T02:57:52.337460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = label_encoder(X_train)\n",
    "X_test = label_encoder(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:54.186131Z",
     "iopub.status.busy": "2021-02-19T02:57:54.185348Z",
     "iopub.status.idle": "2021-02-19T02:57:54.188462Z",
     "shell.execute_reply": "2021-02-19T02:57:54.187957Z"
    },
    "papermill": {
     "duration": 0.020441,
     "end_time": "2021-02-19T02:57:54.188643",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.168202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = KFold(n_splits=5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:54.220860Z",
     "iopub.status.busy": "2021-02-19T02:57:54.220031Z",
     "iopub.status.idle": "2021-02-19T02:57:54.222988Z",
     "shell.execute_reply": "2021-02-19T02:57:54.222293Z"
    },
    "papermill": {
     "duration": 0.021692,
     "end_time": "2021-02-19T02:57:54.223133",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.201441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Parameters\n",
    "# ------------------------------------------------------------------------------\n",
    "N_FOLDS = 10\n",
    "N_ESTIMATORS = 30000\n",
    "SEED = 2021\n",
    "BAGGING_SEED = 48\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# LightGBM: training and inference\n",
    "# ------------------------------------------------------------------------------\n",
    "lgbm_params = {'random_state': SEED,\n",
    "          'metric': 'rmse',\n",
    "          'n_estimators': N_ESTIMATORS,\n",
    "          'n_jobs': -1,\n",
    "          'cat_feature': [x for x in range(len(cat_cols))],\n",
    "          'bagging_seed': SEED,\n",
    "          'feature_fraction_seed': SEED,\n",
    "          'learning_rate': 0.003899156646724397,\n",
    "          'max_depth': 99,\n",
    "          'num_leaves': 63,\n",
    "          'reg_alpha': 9.562925363678952,\n",
    "          'reg_lambda': 9.355810045480153,\n",
    "          'colsample_bytree': 0.2256038826485174,\n",
    "          'min_child_samples': 290,\n",
    "          'subsample_freq': 1,\n",
    "          'subsample': 0.8805303688019942,\n",
    "          'max_bin': 882,\n",
    "          'min_data_per_group': 127,\n",
    "          'cat_smooth': 96,\n",
    "          'cat_l2': 19\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T02:57:54.253133Z",
     "iopub.status.busy": "2021-02-19T02:57:54.252195Z",
     "iopub.status.idle": "2021-02-19T04:27:11.953340Z",
     "shell.execute_reply": "2021-02-19T04:27:11.953891Z"
    },
    "papermill": {
     "duration": 5357.717902,
     "end_time": "2021-02-19T04:27:11.954129",
     "exception": false,
     "start_time": "2021-02-19T02:57:54.236227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Base model is 0.8417355453854034\n",
      "RMSE for Incremental trial 1 model is 0.8417186407974255\n",
      "RMSE for Incremental trial 2 model is 0.8417135133545762\n",
      "RMSE for Incremental trial 3 model is 0.841706269869924\n",
      "RMSE for Incremental trial 4 model is 0.8417002476913066\n",
      "RMSE for Incremental trial 5 model is 0.8416970624752245\n",
      "RMSE for Incremental trial 6 model is 0.8416931611517247\n",
      "RMSE for Incremental trial 7 model is 0.8416884166464895\n",
      "\n",
      "\n",
      "Improvement of : 4.712873891388192e-05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RMSE for Base model is 0.8413132573162314\n",
      "RMSE for Incremental trial 1 model is 0.8413130886575063\n",
      "RMSE for Incremental trial 2 model is 0.8413128719772601\n",
      "RMSE for Incremental trial 3 model is 0.8413117466785476\n",
      "RMSE for Incremental trial 4 model is 0.8413102499838527\n",
      "RMSE for Incremental trial 5 model is 0.841307735709768\n",
      "RMSE for Incremental trial 6 model is 0.8413019772685878\n",
      "RMSE for Incremental trial 7 model is 0.8412952296748583\n",
      "\n",
      "\n",
      "Improvement of : 1.8027641373152825e-05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RMSE for Base model is 0.8427634988175267\n",
      "RMSE for Incremental trial 1 model is 0.8427556220323833\n",
      "RMSE for Incremental trial 2 model is 0.8427507880467284\n",
      "RMSE for Incremental trial 3 model is 0.8427482512257611\n",
      "RMSE for Incremental trial 4 model is 0.842747382425304\n",
      "RMSE for Incremental trial 5 model is 0.8427459283679887\n",
      "RMSE for Incremental trial 6 model is 0.842743290368865\n",
      "RMSE for Incremental trial 7 model is 0.8427409535521937\n",
      "\n",
      "\n",
      "Improvement of : 2.254526533296275e-05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RMSE for Base model is 0.8427096635698776\n",
      "RMSE for Incremental trial 1 model is 0.8427013470404832\n",
      "RMSE for Incremental trial 2 model is 0.8427013300597928\n",
      "RMSE for Incremental trial 3 model is 0.8427006629708059\n",
      "RMSE for Incremental trial 4 model is 0.8426901344393316\n",
      "RMSE for Incremental trial 5 model is 0.8426825662921443\n",
      "RMSE for Incremental trial 6 model is 0.8426754063262216\n",
      "RMSE for Incremental trial 7 model is 0.8426725642120166\n",
      "\n",
      "\n",
      "Improvement of : 3.709935786100349e-05\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RMSE for Base model is 0.8404494116818335\n",
      "RMSE for Incremental trial 1 model is 0.8404491279890197\n",
      "RMSE for Incremental trial 2 model is 0.8404488589274531\n",
      "RMSE for Incremental trial 3 model is 0.8404476988375678\n",
      "RMSE for Incremental trial 4 model is 0.8404457318612615\n",
      "RMSE for Incremental trial 5 model is 0.8404449304192775\n",
      "RMSE for Incremental trial 6 model is 0.8404438256606253\n",
      "RMSE for Incremental trial 7 model is 0.8404424100637806\n",
      "\n",
      "\n",
      "Improvement of : 7.0016180528931216e-06\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preds_list_base = []\n",
    "preds_list_final_iteration = []\n",
    "preds_list_all = []\n",
    "\n",
    "for train_idx, val_idx in split.split(X_train):\n",
    "            X_tr = X_train.iloc[train_idx]\n",
    "            X_val = X_train.iloc[val_idx]\n",
    "            y_tr = y_train.iloc[train_idx]\n",
    "            y_val = y_train.iloc[val_idx]\n",
    "            \n",
    "            Model = LGBMRegressor(**lgbm_params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=250, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0)\n",
    "            \n",
    "            preds_list_base.append(Model.predict(X_test))\n",
    "            preds_list_all.append(Model.predict(X_test))\n",
    "            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            params = lgbm_params.copy()\n",
    "            \n",
    "            for i in range(1, 8):\n",
    "                if i >2:    \n",
    "                    \n",
    "                    # reducing regularizing params if \n",
    "                    \n",
    "                    params['reg_lambda'] *= 0.9\n",
    "                    params['reg_alpha'] *= 0.9\n",
    "                    params['num_leaves'] += 40\n",
    "                    \n",
    "                params['learning_rate'] = 0.003\n",
    "                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n",
    "                          eval_metric=['rmse'],\n",
    "                          early_stopping_rounds=200, \n",
    "                          categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n",
    "                          verbose=0,\n",
    "                          init_model=Model)\n",
    "                \n",
    "                preds_list_all.append(Model.predict(X_test))\n",
    "                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n",
    "            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n",
    "            print('',end='\\n\\n')\n",
    "            print(f'Improvement of : {first_rmse - last_rmse}')\n",
    "            print('-' * 100)\n",
    "            preds_list_final_iteration.append(Model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026481,
     "end_time": "2021-02-19T04:27:12.009023",
     "exception": false,
     "start_time": "2021-02-19T04:27:11.982542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, there are some further improvement in all the folds. Below are some specific findings:\n",
    "\n",
    "* The first few iterations are just using very low learning_rate.. after the 2nd iteration we can see that there are iterations with very good improvement, observed by reducing regularization.\n",
    "* There are also iterations where loss increased at later iterations slightly compared to previous iteration, showing that we have reached the limit in few iterations before the max iteration.\n",
    "* If you try setting verbose=1, you will observe that these improvements are observed only in first few trees created... after that loss starts to increase, LGBM keeps the best model. But reducing regularization does improve loss for the first few trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T04:27:12.292988Z",
     "iopub.status.busy": "2021-02-19T04:27:12.292340Z",
     "iopub.status.idle": "2021-02-19T04:27:12.301260Z",
     "shell.execute_reply": "2021-02-19T04:27:12.300718Z"
    },
    "papermill": {
     "duration": 0.040136,
     "end_time": "2021-02-19T04:27:12.301392",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.261256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.6122141 , 7.77927529, 7.60381899, ..., 7.53936653, 7.47794379,\n",
       "       7.27151483])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_final_iteration = np.array(preds_list_final_iteration).mean(axis=0)\n",
    "y_preds_final_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T04:27:12.360962Z",
     "iopub.status.busy": "2021-02-19T04:27:12.360012Z",
     "iopub.status.idle": "2021-02-19T04:27:12.365974Z",
     "shell.execute_reply": "2021-02-19T04:27:12.366462Z"
    },
    "papermill": {
     "duration": 0.037132,
     "end_time": "2021-02-19T04:27:12.366658",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.329526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# public LB score 0.84192\n",
    "submission = pd.DataFrame({'id':test.id,\n",
    "              'target':y_preds_final_iteration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-19T04:27:12.427651Z",
     "iopub.status.busy": "2021-02-19T04:27:12.426986Z",
     "iopub.status.idle": "2021-02-19T04:27:13.151400Z",
     "shell.execute_reply": "2021-02-19T04:27:13.150862Z"
    },
    "papermill": {
     "duration": 0.75707,
     "end_time": "2021-02-19T04:27:13.151592",
     "exception": false,
     "start_time": "2021-02-19T04:27:12.394522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('gv_extreme_lgb_7_steps.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.028568,
     "end_time": "2021-02-19T04:27:13.410908",
     "exception": false,
     "start_time": "2021-02-19T04:27:13.382340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are done. That is all, folks!\n",
      "Finished at  2021-02-21 13:43:29.518634\n",
      "Elapsed time:  1:47:50.112954\n"
     ]
    }
   ],
   "source": [
    "print('We are done. That is all, folks!')\n",
    "finish_time = dt.datetime.now()\n",
    "print(\"Finished at \", finish_time)\n",
    "elapsed = finish_time - start_time\n",
    "print(\"Elapsed time: \", elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5375.500553,
   "end_time": "2021-02-19T04:27:15.130226",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-19T02:57:39.629673",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
